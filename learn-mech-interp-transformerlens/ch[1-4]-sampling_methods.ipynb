{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "305fdc70-2869-49e8-b960-9e0df0ba3a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformer_lens\n",
    "from transformer_lens import HookedTransformer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9461888e-2cde-4b86-8d50-082fbf4b86b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2 into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = transformer_lens.HookedTransformer.from_pretrained(\"gpt2\")\n",
    "model.eval()\n",
    "tokenizer = model.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "81657729-0b88-4cb1-8e66-5715038e6aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Greedy sampling\n",
    "def gen_greedy(model, tokenizer, seq, max_new_tokens=5):\n",
    "    tokens = model.to_str_tokens(seq)\n",
    "    i = 0\n",
    "    \n",
    "    while i < max_new_tokens:\n",
    "        logits = model(tokens)\n",
    "        last_token_logits = logits[:, 1, :][-1]\n",
    "        soft_logits = torch.softmax(last_token_logits, dim=-1)\n",
    "\n",
    "        next_token = model.to_string(torch.argmax(soft_logits, dim=-1).item())\n",
    "        tokens.append(next_token)\n",
    "        i += 1\n",
    "        \n",
    "    return \"\".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9cb73b18-4967-4db4-b85c-3e88ec998d1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>jack and jill went up the hill. jack game some of the first,\\nThe first,\\nThe'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_greedy(model, tokenizer, 'jack and jill went up the hill. jack game some', max_new_tokens=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "80e330fb-cdf3-4e0b-a7e1-71cce041af06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature based\n",
    "def gen_temp(model, tokenizer, seq, max_new_tokens=5, temperature=1):\n",
    "    tokens = model.to_str_tokens(seq)\n",
    "    i = 0\n",
    "    \n",
    "    while i < max_new_tokens:\n",
    "        logits = model(tokens)      \n",
    "        last_token_logits = logits[:, 1, :][-1]\n",
    "        soft_logits = torch.softmax(last_token_logits/temperature, dim=-1)\n",
    "\n",
    "        next_token = model.to_string(torch.multinomial(soft_logits, num_samples=1).item())\n",
    "        tokens.append(next_token)\n",
    "        i += 1\n",
    "        \n",
    "    return \"\".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bdda4319-99b4-4134-9be5-c25ddd05f8fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>jack and jill went up the hill. jack gave some of the \"I am I\\'m so that is'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_temp(model, tokenizer, \"jack and jill went up the hill. jack gave some\", max_new_tokens=10, temperature=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3a45ccfb-8158-4780-8369-8401d0c3b11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top-k search\n",
    "def top_k_search(model, tokenizer, seq, max_new_tokens=5, top_k=5, temperature=0.8):\n",
    "    tokens = model.to_str_tokens(seq)\n",
    "    i = 0\n",
    "    \n",
    "    while i < max_new_tokens:\n",
    "        logits = model(tokens)      \n",
    "        last_token_logits = logits[:, 1, :][-1]\n",
    "        \n",
    "        soft_topk, topk_idx = torch.topk(last_token_logits, k=top_k)\n",
    "        soft_logits = torch.softmax(soft_topk/temperature, dim=-1)\n",
    "        chosen_idx = torch.multinomial(soft_logits, num_samples=1).item()\n",
    "        next_token_id = topk_idx[chosen_idx].item()\n",
    "        \n",
    "        tokens.append(model.to_string(next_token_id))\n",
    "        i += 1\n",
    "        \n",
    "    return \"\".join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "08e19adc-fc1c-469e-ad53-5fb32dfb5124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>jack and jill went up the hill. jack gave some people who was not a'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_k_search(model, tokenizer, \"jack and jill went up the hill. jack gave some\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8e9b4b-8cee-447a-b2c4-a78870b7a5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beam search [incomplete]\n",
    "def beam_search(model, tokenizer, seq, max_new_tokens=5, n_beam=3):\n",
    "    tokens = model.to_str_tokens(seq)\n",
    "    i = 0\n",
    "    \n",
    "    while i < max_new_tokens:\n",
    "        beam_count = 0\n",
    "        while n_beams:\n",
    "            logits = model(tokens)    \n",
    "            last_token_logits = logits[:, 1, :][-1]\n",
    "            soft_logits = torch.softmax(last_token_logits, dim=-1)\n",
    "            \n",
    "            next_token = model.to_string(torch.multinomial(soft_logits, num_samples=1).item())\n",
    "            tokens.append(next_token)\n",
    "        i += 1\n",
    "        \n",
    "    return \"\".join(tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
