{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "931aae31-4e86-4b19-8f43-b75d14763897",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "from einops import einsum\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb7dc8d9-9b57-472e-97f1-21849e559fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sivakrishna/miniconda3/envs/test/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model tiny-stories-1M into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\"tiny-stories-1M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb81b91-4a5d-4f66-a0e7-690c36dd31ed",
   "metadata": {},
   "source": [
    "# RUNNING MODEL WITH ACTIVATIONCACHE #\n",
    "# running with activations - returns activation values collected durning inference #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d1520a7-39e0-42f6-9c11-c9ddd5335cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, cache = model.run_with_cache(\"Why did the chicken cross the\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ba30b07-ebe1-4d41-ae39-f9deb50ac4c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 628,  750,  345, 1310,  467,   30, 2323], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['\\n\\n', ' did', ' you', ' little', ' go', '?', ' ground']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(logits.argmax(dim=-1).squeeze())\n",
    "[model.to_string(x) for x in logits.argmax(dim=-1).squeeze()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd11b390-b9c7-41c3-aae0-08100b576ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hook_embed', 'hook_pos_embed', 'blocks.0.hook_resid_pre', 'blocks.0.ln1.hook_scale', 'blocks.0.ln1.hook_normalized', 'blocks.0.attn.hook_q', 'blocks.0.attn.hook_k', 'blocks.0.attn.hook_v', 'blocks.0.attn.hook_attn_scores', 'blocks.0.attn.hook_pattern']\n"
     ]
    }
   ],
   "source": [
    "\"\"\" ActivationCache - dictionary of activation tensors of different layers from different blocks \"\"\" \n",
    "\n",
    "print(list(cache.keys())[:10])    # total keys(hooks) = approx 140\n",
    "# print(cache.values())\n",
    "# print(cache.items())   # (key, value) tuples\n",
    "# print(cache.cache_dict[\"hook_embed\"])   # cache dictionary (keys: values) \"access individual activation this way!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea3a737-115d-42df-81ed-394c08bd8e97",
   "metadata": {},
   "source": [
    "# DECOMPOSING THE RESIDUAL STREAM #\n",
    "# Decomposes the residual stream - returns the layer outputs that are being added into the residual stream #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d168b004-5b27-4d92-b7d0-01a86028476c",
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposed_stream, lables = cache.decompose_resid(return_labels=True, mode=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5c06c2c-b462-4a62-80f6-014fa762d755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([18, 1, 7, 64])\n"
     ]
    }
   ],
   "source": [
    "print(decomposed_stream.size())   # (n_lables, batch_size, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6139127b-bf8f-4171-839c-d112ce1cab70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['embed', 'pos_embed', '0_attn_out', '0_mlp_out', '1_attn_out', '1_mlp_out', '2_attn_out', '2_mlp_out', '3_attn_out', '3_mlp_out', '4_attn_out', '4_mlp_out', '5_attn_out', '5_mlp_out', '6_attn_out', '6_mlp_out', '7_attn_out', '7_mlp_out']\n"
     ]
    }
   ],
   "source": [
    "print(lables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6de9050-4bc9-451f-9ba0-0e59ac75dbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Takes a residual stack (typically the residual stream decomposed by components), and calculates how\n",
    "much each item in the stack \"contributes\" to specific tokens.\n",
    "\n",
    "    It does this by:\n",
    "        1. Getting the residual directions of the tokens (i.e. reversing the unembed)\n",
    "        2. Taking the dot product of each item in the residual stack (activations), with the token residual\n",
    "            directions.\n",
    "            \n",
    "    <<< Algorithm >>>\n",
    "    \n",
    "        logit_directions = tokens_to_residual_directions(tokens)   # map tokens to a tensor with the unembedding vector for those tokens.\n",
    "        scaled_directions = layer_norm(logit_directions)\n",
    "        layer_attrs = (scaled_directions * logit_direction).sum(dim=-1)\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "assumed_prediction = \" road\"   # our token which is a \"close possibility\"\n",
    "logit_attributions = cache.logit_attrs(decomposed_stream, assumed_prediction)   # calculating logit attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "feae8bd3-9591-44af-99a6-bbb65eb3eed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([18, 1, 7])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_attributions.size()  # (n_layers, batch_size, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3215699-80e8-42da-83e0-46fae74b2b4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 49.0849,   6.0892, -21.5946,  49.0395, -37.6695,   9.1405, -49.2915,\n",
       "          8.4667,   1.7180,  10.5615, -10.4780,  20.0837, -19.1105,  23.8754,\n",
       "        -33.9309,  13.6613, -13.0773,  13.5248], device='cuda:0',\n",
       "       grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_attributions.squeeze().sum(dim=1)   # attribution of each layer towards the assumed_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e25d636a-e9f9-4f61-91a2-00e2291ddfb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1_mlp_out\n"
     ]
    }
   ],
   "source": [
    "most_imp_component_idx = logit_attributions.argmax()\n",
    "print(lables[most_imp_component_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c8a8a32-d652-4ebe-becf-33240c95f0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    }
   ],
   "source": [
    "full_resid, labels = cache.get_full_resid_decomposition(return_labels=True)   # for more granular decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75061400-53d0-4d3c-83b2-45c91f4b5e5c",
   "metadata": {},
   "source": [
    "# ACCUMULATING THE RESIDUAL STREAM #\n",
    "# accumulating residual stream - returns residual stream after every block #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6f471b99-e97b-4a00-8fa7-e882f3dc8ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_resid, acc_labels = cache.accumulated_resid(layer=8, incl_mid=False, apply_ln=True, mlp_input=False, return_labels=True)\n",
    "# useful parameters - layer(int), incl_mid(bool), apply_ln(bool), mlp_input(bool), return_labels(bool)\n",
    "# incl_mid ==> returns \"resid_mid\" for all previous layers.\n",
    "# mlp_input ==> whether to include \"resid_mid\" for the current layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "925df842-bfe5-41fc-ad35-ad6deabbe6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 1, 7, 64])\n",
      "['0_pre', '1_pre', '2_pre', '3_pre', '4_pre', '5_pre', '6_pre', '7_pre', 'final_post']\n"
     ]
    }
   ],
   "source": [
    "print(acc_resid.size())\n",
    "print(acc_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b9406736-d050-4658-99e7-2c86a190345f",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_token_accum = acc_resid[:, 0, -1, :]   # accumulated stream values for the last token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8fdb5557-3410-4132-9a27-6311cf9721c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 64])\n",
      "torch.Size([64, 50257])\n"
     ]
    }
   ],
   "source": [
    "print(last_token_accum.size())\n",
    "print(model.W_U.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "06e9497a-e659-4011-b576-965002519a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_unembedded = einsum(\n",
    "    last_token_accum,\n",
    "    model.W_U,\n",
    "    \"layer d_model, d_model d_vocab -> layer d_vocab\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e984cc8d-cf14-417b-84eb-efad54fb91cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_indices = torch.argsort(layers_unembedded, dim=1, descending=True)\n",
    "rank_answer = (sorted_indices == model.to_single_token(\" ground\")).nonzero(as_tuple=True)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "75c344dd-effa-4a48-997e-bdc6c53a49e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1999,  297,  124,  177,    9,   11,    3,   17,    2], device='cuda:0')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank_answer   # rank of last token at each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4b8f7c-b3f6-4713-9202-410c0b3dae56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CLASS:\n",
    "class transformer_lens.ActivationCache.ActivationCache(cache_dict: Dict[str, Tensor], model, has_batch_dim: bool = True)\n",
    "\n",
    "METHODS:\n",
    "accumulated_resid\n",
    "apply_ln_to_stack\n",
    "apply_slice_to_batch_dim\n",
    "compute_head_results\n",
    "decompose_resid\n",
    "get_full_resid_decomposition\n",
    "get_neuron_results\n",
    "items\n",
    "keys\n",
    "values\n",
    "logit_attrs\n",
    "remove_batch_dim\n",
    "stack_activation\n",
    "stack_head_results\n",
    "stack_neuron_results\n",
    "to\n",
    "toggle_autodiff\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
